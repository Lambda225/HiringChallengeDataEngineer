# Hiring Challenge DataEngineer

## üöÄ Description

Ce projet met en place un **pipeline ETL (Extract, Transform, Load) automatis√©** avec Apache Airflow, permettant l‚Äôextraction, la transformation et le chargement des **donn√©es provenant de deux capteurs de surveillance de qualit√© de l'air** dans **MongoDB**.
Les donn√©es collect√©es sont ensuite accessibles via **Apache Drill**, offrant une interface **SQL** pour interroger **MongoDB**, et sont visualis√©es dans **Apache Superset** pour une analyse et un suivi en temps r√©el de la qualit√© de l'air.

## üèóÔ∏è Technologies utilis√©es

- **Apache Airflow** : Orchestration du pipeline ETL
- **MongoDB** : Stockage des donn√©es
- **Apache Drill** : Acc√®s SQL aux donn√©es MongoDB
- **Apache Superset** : Visualisation et exploration des donn√©es

## ‚ö° Fonctionnalit√©s

‚úîÔ∏è Extraction et transformation des donn√©es  
‚úîÔ∏è Chargement automatique dans MongoDB  
‚úîÔ∏è Connexion et interrogation SQL via Apache Drill  
‚úîÔ∏è Tableaux de bord interactifs avec Superset

## üõ†Ô∏è Installation & Ex√©cution

**NB :** _Avant de poursuivre, il est important de noter que toutes les op√©rations ont √©t√© r√©alis√©es sur un syst√®me Linux. Si vous utilisez un autre syst√®me d'exploitation, vous devrez adapter les commandes en cons√©quence. Assurez-vous d'avoir d√©j√† install√© **MongoDB**, **Python** et **Java**._

1. **Clone le projet et acc√®de** :
   ```sh
   git clone https://github.com/Lambda225/HiringChallengeDataEngineer.git
   cd HiringChallengeDataEngineer
   ```
2. **Cr√©e un environnement virtul et activation**

   ```sh
    python3 -m venv .venv
    source .venv/bin/activate
   ```

3. **Installe les d√©pandance python**
   ```sh
   pip install -r requirements.txt
   ```
4. **D√©finit les variables d'environnement**

   cr√©er une fichier nomm√© `.env` a la racine du repectoire et ajouter les variables suvantes en modifiant les valeurs:

   ```.env
   STATION_ID = keyStation1,keyStation1
   DB_URL = yourDataBaseUrl
   ```

5. **lance apache Airflow et excecute le dag**

   ```sh
   export AIRFLOW_HOME=projectPath
   airflow db init
   airflow users create --username admin --firstname firstname --lastname lastname --role Admin --email admin@domain.com
   ```

   apr√®s ses √©tapes vous serez amen√© a entrer un mots de passe.Juste apr√®s cela veiller ex√©cuter ses commande

   ```sh
   airflow standalone
   ```

   acc√©dez a l'addresse http://localhost:8080 pour vous connect√© a airflow et activ√© le dag _ETL_airquality_

   **NB** : _Avant cela assurer vous aurez vous que mongoDB est lanc√©_

   ![activation du dag](./image/active_dag.png)

6. **Connect mongoDB a apache Drill**

   ```sh
   ./apache-drill-1.21.2/bin/drill-embedded
   ```

   acc√©dez a l'addresse http://localhost:8047 puis allez dans storage. Ensuite click√© sur enable en face de mongo. Allez dans query entrer les codes suivant pour vous assurer que la connection a √©t√© √©ffectuer

   ![activation du connecteur](./image/active_drill_connector.png)

   ```sql
   SELECT * FROM mongo.airquality.sensor
   ```

   **NB :** Vous pouvez toujours modier url de connection si celui par d√©fault ne correspond pas

7. **Visualise avec Apache Superset**

   ```sh
   export FLASK_APP=superset
   export SUPERSET_CONFIG_PATH=yourProjectPath/superset_config.py
   superset db upgrade
   superset fab create-admin
   ```

   apr√®s ses √©tapes vous serais amenez a cr√©er votre utilisateur superset

   ```sh
   superset init
   ```

   rajouter le chemain vers le fichier `activate` de votre environement virtule √† la trois√®me ligne du fichier `run_superset.sh`

   ```sh
   ./run_superset
   ```

   rendez-vous √† l'addresse http://localhost:8088
   pour acc√©der √† apache superset

## ‚è≥ Comment Apache Airflow r√©cup√®re les donn√©es chaque heure ?

Dans ce projet, nous avons eu √† notre disposition une **API** fournissant des donn√©es issues de **deux stations de surveillance** qui mesurent, chaque heure, des informations sur la **qualit√© de l'air** sur une p√©riode d‚Äôun an.

L‚ÄôETL que nous avons con√ßu suit les √©tapes suivantes :

---

### 1Ô∏è. **Extraction (Extract)**

- R√©cup√©ration des donn√©es brutes provenant des **deux capteurs de qualit√© de l‚Äôair** via une API REST.
- Extraction des mesures de **temp√©rature, PM2.5, CO, et autres polluants atmosph√©riques**.

---

### 2. **V√©rification de l'existence de la base de donn√©es**

Avant de proc√©der √† la transformation des donn√©es, nous nous connectons √† **MongoDB** et v√©rifions si la base de donn√©es `airquality` existe d√©j√†.

---

### 3Ô∏è. **Transformation (Transform)**

üîπ **Pr√©-traitement des donn√©es** :

- Conversion de la variable **`timestamp`** en format date.
- Renommage de certaines variables pour assurer la coh√©rence des donn√©es.

üîπ **Deux cas de figure se pr√©sentent :**

1Ô∏è. **Si la base de donn√©es `airquality` n'existe pas** :

- Calcul de la **moyenne journali√®re** des variables **CO et PM2.5**, sans prendre en compte la journ√©e en cours si elle n'est pas termin√©e.

2Ô∏è. **Si la base de donn√©es `airquality` existe** :

- R√©cup√©ration de la **derni√®re valeur enregistr√©e** par chaque capteur.
- Mise √† jour de la **moyenne journali√®re** des variables CO et PM2.5 si nous sommes en fin de journ√©e.

---

### 4Ô∏è. **Chargement (Load)**

- Stockage des donn√©es transform√©es dans **MongoDB**.
- Indexation pour une interrogation rapide via **Apache Drill**.
- Int√©gration des donn√©es dans **Apache Superset** pour la visualisation et l'analyse.

## üöÄ Mise en Production du Pipeline ETL avec Apache Airflow

Pour mettre en production ce projet, je vous recommande de cr√©er une **machine virtuelle Linux** propos√©e par un **fournisseur cloud** (ex: AWS, Azure, GCP).

Vous devrez √©galement disposer d'un **compte MongoDB**, soit via une **instance autog√©r√©e** (MongoDB install√© sur votre VM) ou un **service manag√©** comme **MongoDB Atlas**.

Une fois la machine virtuelle cr√©√©e, **acc√©dez √† la console** via **SSH** et suivez les √©tapes d√©finies dans la section suivante :

üîó [üõ†Ô∏è Installation & Ex√©cution](#-installation--ex√©cution)

üìå **Pr√©requis recommand√©s** :

- Une machine virtuelle avec **Ubuntu 20.04+** ou **Debian**.
- Un acc√®s **root** ou un utilisateur avec les permissions `sudo`.
- Une connexion Internet stable pour installer les d√©pendances.
- Un compte **MongoDB Atlas** ou une base **MongoDB locale**.

Une fois connect√© √† la machine virtuelle, vous pourrez proc√©der aux installations et configurations n√©cessaires pour ex√©cuter le pipeline ETL avec **Apache Airflow**.

## üéØ **Conclusion**

Ce projet a √©t√© une **exp√©rience enrichissante**, permettant d‚Äôacqu√©rir et de renforcer plusieurs comp√©tences cl√©s dans le domaine du **Big Data, de l'orchestration des workflows et de la visualisation des donn√©es**.

Gr√¢ce √† la mise en place d‚Äôun **pipeline ETL automatis√©** avec **Apache Airflow**, nous avons appris √† orchestrer efficacement l‚Äôextraction, la transformation et le chargement des donn√©es issues de **capteurs de surveillance de la qualit√© de l'air**.

L‚Äôint√©gration avec **MongoDB** nous a permis de mieux comprendre le stockage et la gestion des bases de donn√©es **NoSQL**, tandis que **Apache Drill** nous a familiaris√©s avec l'interrogation des bases de donn√©es **semi-structur√©es** via SQL. Enfin, la visualisation des donn√©es avec **Apache Superset** a renforc√© nos comp√©tences en **analyse de donn√©es et en business intelligence**.
