# Hiring Challenge DataEngineer

## üöÄ Description

Ce projet met en place un **pipeline ETL (Extract, Transform, Load) automatis√©** avec Apache Airflow, permettant l‚Äôextraction, la transformation et le chargement des **donn√©es provenant de deux capteurs de surveillance de qualit√© de l'air** dans **MongoDB**.
Les donn√©es collect√©es sont ensuite accessibles via **Apache Drill**, offrant une interface **SQL** pour interroger **MongoDB**, et sont visualis√©es dans **Apache Superset** pour une analyse et un suivi en temps r√©el de la qualit√© de l'air.

## üèóÔ∏è Technologies utilis√©es

- **Apache Airflow** : Orchestration du pipeline ETL
- **MongoDB** : Stockage des donn√©es
- **Apache Drill** : Acc√®s SQL aux donn√©es MongoDB
- **Apache Superset** : Visualisation et exploration des donn√©es

## ‚ö° Fonctionnalit√©s

‚úîÔ∏è Extraction et transformation des donn√©es  
‚úîÔ∏è Chargement automatique dans MongoDB  
‚úîÔ∏è Connexion et interrogation SQL via Apache Drill  
‚úîÔ∏è Tableaux de bord interactifs avec Superset

## üõ†Ô∏è Installation & Ex√©cution

**NB :** _Avant de poursuivre, il est important de noter que toutes les op√©rations ont √©t√© r√©alis√©es sur un syst√®me Linux. Si vous utilisez un autre syst√®me d'exploitation, vous devrez adapter les commandes en cons√©quence. Assurez-vous d'avoir d√©j√† install√© **MongoDB**, **Python** et **Java**._

---

1. **Cloner le Projet et Acc√©der au R√©pertoire** :
   ```sh
   git clone https://github.com/Lambda225/HiringChallengeDataEngineer.git
   cd HiringChallengeDataEngineer
   ```
2. **Cr√©er et Activer un Environnement Virtuel**

   ```sh
    python3 -m venv .venv
    source .venv/bin/activate
   ```

3. **Installer les D√©pendances Python**
   ```sh
   pip install -r requirements.txt
   ```
4. **D√©finir les Variables d'Environnement**

   Cr√©ez un fichier nomm√© .env √† la racine du r√©pertoire et ajoutez-y les variables suivantes en modifiant leurs valeurs :

   ```.env
   STATION_ID = keyStation1,keyStation1
   DB_URL = yourDataBaseUrl
   ```

5. **Lancer Apache Airflow et Ex√©cuter le DAG**

   D√©finissez la variable d'environnement AIRFLOW_HOME :

   ```sh
   export AIRFLOW_HOME=projectPath
   ```

   Initialisez la base de donn√©es d'Airflow et cr√©ez un utilisateur administrateur :

   ```sh
   airflow db init
   airflow users create --username admin --firstname firstname --lastname lastname --role Admin --email admin@domain.com
   ```

   Apr√®s avoir entr√© le mot de passe, d√©marrez Airflow en mode autonome :

   ```sh
   airflow standalone
   ```

   Acc√©dez ensuite √† http://localhost:8080 pour vous connecter √† Airflow et activer le DAG _ETL_airquality_.

   **NB** : _Assurez-vous que MongoDB est bien lanc√© avant de d√©marrer Airflow._

   ![activation du dag](./image/active_dag.png)

6. **Connecter MongoDB √† Apache Drill**
   Lancez Apache Drill

   ```sh
   ./apache-drill-1.21.2/bin/drill-embedded
   ```

   Ensuite, acc√©dez √† http://localhost:8047, allez dans Storage, puis cliquez sur Enable en face de mongo.

   Dans l'onglet Query, ex√©cutez la requ√™te suivante pour v√©rifier que la connexion est bien √©tablie :

   ```sql
   SELECT * FROM mongo.airquality.sensor
   ```

   **NB :** Vous pouvez modifier l'URL de connexion si celle par d√©faut ne correspond pas √† votre configuration.

   ![activation du connecteur](./image/active_drill_connector.png)

7. **Visualiser les Donn√©es avec Apache Superset**

   D√©finissez les variables d'environnement pour Superset :

   ```sh
   export FLASK_APP=superset
   export SUPERSET_CONFIG_PATH=yourProjectPath/superset_config.py
   ```

   Modifiez la variable `SQLALCHEMY_DATABASE_URI` dans `superset_config.py` pour lui indiquer le chemin correct, puis ex√©cutez les commandes suivantes :

   ```sh
   superset db upgrade
   superset fab create-admin
   ```

   Apr√®s ces √©tapes, vous serez invit√© √† cr√©er votre utilisateur Superset.

   ```sh
   superset init
   ```

   Enfin, lancez Apache Superset :

   ```sh
   superset run -p 8088 --with-threads --reload --debugger
   ```

   Acc√©dez √† http://localhost:8088, puis allez dans l'onglet **Dashboards** et importez le fichier `dashboard_export_20250202T221928.zip`.

   **OPTIONNEL : Personnalisation du Dashboard**

   Pour personnaliser le dashboard, d√©placez le dossier `data354` dans :

   ```sh
   .venv/lib/python3.10/site-packages/superset/static/assets/images
   ```

   Ensuite, d√©commentez les trois derni√®res lignes du fichier `superset_config.py`.

## ‚è≥ Comment Apache Airflow r√©cup√®re les donn√©es chaque heure ?

Dans ce projet, nous avons eu √† notre disposition une **API** fournissant des donn√©es issues de **deux stations de surveillance** qui mesurent, chaque heure, des informations sur la **qualit√© de l'air** sur une p√©riode d‚Äôun an.

L‚ÄôETL que nous avons con√ßu suit les √©tapes suivantes :

---

### 1Ô∏è. **Extraction (Extract)**

- R√©cup√©ration des donn√©es brutes provenant des **deux capteurs de qualit√© de l‚Äôair** via une API REST.
- Extraction des mesures de **temp√©rature, PM2.5, CO, et autres polluants atmosph√©riques**.

---

### 2. **V√©rification de l'existence de la base de donn√©es**

Avant de proc√©der √† la transformation des donn√©es, nous nous connectons √† **MongoDB** et v√©rifions si la base de donn√©es `airquality` existe d√©j√†.

---

### 3Ô∏è. **Transformation (Transform)**

üîπ **Pr√©-traitement des donn√©es** :

- Conversion de la variable **`timestamp`** en format date.
- Renommage de certaines variables pour assurer la coh√©rence des donn√©es.

üîπ **Deux cas de figure se pr√©sentent :**

1Ô∏è. **Si la base de donn√©es `airquality` n'existe pas** :

- Calcul de la **moyenne journali√®re** des variables **CO et PM2.5**, sans prendre en compte la journ√©e en cours si elle n'est pas termin√©e.

2Ô∏è. **Si la base de donn√©es `airquality` existe** :

- R√©cup√©ration de la **derni√®re valeur enregistr√©e** par chaque capteur.
- Mise √† jour de la **moyenne journali√®re** des variables CO et PM2.5 si nous sommes en fin de journ√©e.

---

### 4Ô∏è. **Chargement (Load)**

- Stockage des donn√©es transform√©es dans **MongoDB**.
- Indexation pour une interrogation rapide via **Apache Drill**.
- Int√©gration des donn√©es dans **Apache Superset** pour la visualisation et l'analyse.

## üöÄ Mise en Production du Pipeline ETL avec Apache Airflow

Pour mettre en production ce projet, je vous recommande de cr√©er une **machine virtuelle Linux** propos√©e par un **fournisseur cloud** (ex: AWS, Azure, GCP).

Vous devrez √©galement disposer d'un **compte MongoDB**, soit via une **instance autog√©r√©e** (MongoDB install√© sur votre VM) ou un **service manag√©** comme **MongoDB Atlas**.

Une fois la machine virtuelle cr√©√©e, **acc√©dez √† la console** via **SSH** et suivez les √©tapes d√©finies dans la section suivante :

üîó [üõ†Ô∏è Installation & Ex√©cution](#-installation--ex√©cution)

üìå **Pr√©requis recommand√©s** :

- Une machine virtuelle avec **Ubuntu 20.04+** ou **Debian**.
- Un acc√®s **root** ou un utilisateur avec les permissions `sudo`.
- Une connexion Internet stable pour installer les d√©pendances.
- Un compte **MongoDB Atlas** ou une base **MongoDB locale**.

Une fois connect√© √† la machine virtuelle, vous pourrez proc√©der aux installations et configurations n√©cessaires pour ex√©cuter le pipeline ETL avec **Apache Airflow**.

## üéØ **Conclusion**

Ce projet a √©t√© une **exp√©rience enrichissante**, permettant d‚Äôacqu√©rir et de renforcer plusieurs comp√©tences cl√©s dans le domaine du **Big Data, de l'orchestration des workflows et de la visualisation des donn√©es**.

Gr√¢ce √† la mise en place d‚Äôun **pipeline ETL automatis√©** avec **Apache Airflow**, nous avons appris √† orchestrer efficacement l‚Äôextraction, la transformation et le chargement des donn√©es issues de **capteurs de surveillance de la qualit√© de l'air**.

L‚Äôint√©gration avec **MongoDB** nous a permis de mieux comprendre le stockage et la gestion des bases de donn√©es **NoSQL**, tandis que **Apache Drill** nous a familiaris√©s avec l'interrogation des bases de donn√©es **semi-structur√©es** via SQL. Enfin, la visualisation des donn√©es avec **Apache Superset** a renforc√© nos comp√©tences en **analyse de donn√©es et en business intelligence**.
